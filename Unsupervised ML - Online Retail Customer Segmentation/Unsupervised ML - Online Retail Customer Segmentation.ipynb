{"cells":[{"cell_type":"markdown","metadata":{"id":"vncDsAP0Gaoa"},"source":["# **Project Name**    -\n","\n"]},{"cell_type":"markdown","metadata":{"id":"beRrZCGUAJYm"},"source":["##### **Project Type**    - Unsupervised (Clustering)\n","##### **Contribution**    - Individual\n","##### **Contributor **   - Prashant Pratap Singh\n"]},{"cell_type":"markdown","metadata":{"id":"FJNUwmbgGyua"},"source":["# **Project Summary -**"]},{"cell_type":"markdown","metadata":{"id":"F6v_1wHtG2nS"},"source":["In this project we have online retail dataset having information about the customer purchases. Our task here is to extract as such information as we can about our customers to make our business strategies. For this purpose we will divide our dataset into certain number of groups(clusters) in which each group will have customers having similar characteristics. These characteristics can be gender, age, monetary value of purchases, frequency of purchases, recency of purchase etc. Through clustering we can make our marketing strategies more targeted and efficient resulting in better business prospects. Each group(cluster) will have specific marketing strategies based on their characteristics. Our project will thus help improve the business of the company.   "]},{"cell_type":"markdown","metadata":{"id":"yQaldy8SH6Dl"},"source":["# **Problem Statement**\n"]},{"cell_type":"markdown","metadata":{"id":"DpeJGUA3kjGy"},"source":["Our task here is to extract as much information as we can about our customers through the online retail dataset. For this purpose we have to divide our dataset into different clusters(groups) based on certain characteristics\n","These groups will then be used to make targeted marketing startegies"]},{"cell_type":"markdown","metadata":{"id":"O_i_v8NEhb9l"},"source":["# ***Let's Begin !***"]},{"cell_type":"markdown","metadata":{"id":"HhfV-JJviCcP"},"source":["## ***1. Know Your Data***"]},{"cell_type":"markdown","metadata":{"id":"Y3lxredqlCYt"},"source":["### Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8Vqi-pPk-HR"},"outputs":[],"source":["# Import Libraries\n","import numpy as np # for numerical calculations\n","import pandas as pd # for data analysis\n","import matplotlib.pyplot as plt # for data visualization\n","import seaborn as sns # for data visualization\n","import datetime as dt #for date manipulation\n","from numpy import math #for mathematical calculations\n","from sklearn.preprocessing import StandardScaler #for scaling the data\n","from sklearn.metrics import silhouette_score #for clustering evaluation\n","from sklearn.cluster import KMeans #for clustering"]},{"cell_type":"markdown","metadata":{"id":"3RnN4peoiCZX"},"source":["### Dataset Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4CkvbW_SlZ_R"},"outputs":[],"source":["# Load Dataset\n","df = pd.read_csv(\"Online Retail.csv\")"]},{"cell_type":"markdown","metadata":{"id":"x71ZqKXriCWQ"},"source":["### Dataset First View"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"LWNFOSvLl09H"},"outputs":[],"source":["# Dataset First Look\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"7hBIi_osiCS2"},"source":["### Dataset Rows & Columns count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kllu7SJgmLij"},"outputs":[],"source":["# Dataset Rows & Columns count\n","df.shape"]},{"cell_type":"markdown","metadata":{"id":"osrsyjDS1eYS"},"source":["Our dataset has 541909 rows and 8 columns\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JlHwYmJAmNHm"},"source":["### Dataset Information"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9hRXRi6meOf"},"outputs":[],"source":["# Dataset Info\n","df.info()"]},{"cell_type":"markdown","metadata":{"id":"uqSK17qM6ohs"},"source":["Description and CustomerId have null values\n","\n"]},{"cell_type":"markdown","metadata":{"id":"35m5QtbWiB9F"},"source":["#### Duplicate Values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sLdpKYkmox0"},"outputs":[],"source":["# Dataset Duplicate Value Count\n","df.duplicated().sum()"]},{"cell_type":"markdown","metadata":{"id":"2hdnGBcY7K9G"},"source":["Our dataset have 5268 duplicate entries"]},{"cell_type":"markdown","metadata":{"id":"PoPl-ycgm1ru"},"source":["#### Missing Values/Null Values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgHWkxvamxVg"},"outputs":[],"source":["# Missing Values/Null Values Count\n","print(f\"Total number of null values in the dataset is {df.isnull().sum().sum()}\\n\")\n","df_null = df.isnull().sum()\n","df_null"]},{"cell_type":"markdown","metadata":{"id":"HxKeIzJg7-Sy"},"source":["There are 136534 null values with customerID having the most null values(135080)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3q5wnI3om9sJ"},"outputs":[],"source":["# Visualizing the missing values\n","plt.figure(figsize = (10,10))\n","sns.barplot(x = df_null.index, y = df_null.values)\n","plt.xlabel(\"Columns\")\n","plt.ylabel(\"Number of Missing Values\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"H0kj-8xxnORC"},"source":["### What did you know about your dataset?"]},{"cell_type":"markdown","metadata":{"id":"gfoNAAC-nUe_"},"source":["Our dataset have -\n","\n","1. 541909 rows , 8 columns\n","2. 5268 duplicate values\n","3. 136534 missing values"]},{"cell_type":"markdown","metadata":{"id":"nA9Y7ga8ng1Z"},"source":["## ***2. Understanding Your Variables***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j7xfkqrt5Ag5"},"outputs":[],"source":["# Dataset Columns\n","df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"DnOaZdaE5Q5t"},"outputs":[],"source":["# Dataset Describe\n","df.describe()"]},{"cell_type":"markdown","metadata":{"id":"PBTbrJXOngz2"},"source":["### Variables Description"]},{"cell_type":"markdown","metadata":{"id":"aJV4KIxSnxay"},"source":["1. InvoiceNo - Invoice Number\n","2. StockCode - Stock Name Code\n","3. Description - Description of Product\n","4. Quantity - Quantity Purchased\n","5. InvoiceDate - Date of Purchase\n","6. UnitPrice - Price of one unit\n","7. CustomerID - Unique Id of Customer\n","8. Country - Country of Customer"]},{"cell_type":"markdown","metadata":{"id":"u3PMJOP6ngxN"},"source":["### Check Unique Values for each variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zms12Yq5n-jE"},"outputs":[],"source":["# Check Unique Values for each variable.\n","df.nunique()"]},{"cell_type":"markdown","metadata":{"id":"IxAkleF6JR6Y"},"source":["We can check the unique values for each variable here. InvoiceNo has the most unique values followed by InvoiceDate"]},{"cell_type":"markdown","metadata":{"id":"dauF4eBmngu3"},"source":["## 3. ***Data Wrangling***"]},{"cell_type":"markdown","metadata":{"id":"bKJF3rekwFvQ"},"source":["### Data Wrangling Code"]},{"cell_type":"markdown","metadata":{"id":"9u7J0jC6KBOr"},"source":["**1. Handling Missing Values**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wk-9a2fpoLcV"},"outputs":[],"source":["#removing all rows with null values in certain columns\n","\n","df.dropna(subset = ['InvoiceNo','Quantity', 'InvoiceDate',\n","       'UnitPrice', 'CustomerID', 'Country'], inplace = True )\n","\n","df.shape #checking number of rows after removal"]},{"cell_type":"markdown","metadata":{"id":"LqpSqifniEPh"},"source":["Now there are only 406829 rows after removing all the null value rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehS9yC6_h7og"},"outputs":[],"source":["#checking number of null values in each variable\n","df.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"Qakga3DOihTM"},"source":["Now there are no variables with null values"]},{"cell_type":"markdown","metadata":{"id":"jOfZs2H_i7VW"},"source":["**2. Removing Duplicate Entries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_XyOABnjGJW"},"outputs":[],"source":["# finding number of duplicate entries\n","df.duplicated().sum()"]},{"cell_type":"markdown","metadata":{"id":"GoqoWibskBEa"},"source":["We have to now remove the duplicate entries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_WEEC8kkYLK"},"outputs":[],"source":["#removing duplicates\n","df.drop_duplicates(inplace = True)\n","df.duplicated().sum()"]},{"cell_type":"markdown","metadata":{"id":"f0PkSuoQk1UQ"},"source":["Now there are no duplicates left in the dataset"]},{"cell_type":"markdown","metadata":{"id":"pMW6OMiclZVY"},"source":["**3. Removing Outliers**"]},{"cell_type":"markdown","metadata":{"id":"udi4nWN9n0bq"},"source":["We will remove outliers in Quantity and Unit Price variables through IQR method.\n","IQR is the difference between the value at 75 percentile to the value at 25 percentile. We found these percentile values through describe method above"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"syy-W8Vzp_t2"},"outputs":[],"source":["#removing outliers in quantity variable\n","percentile_75 = 10\n","percentile_25 = 1\n","\n","IQR = percentile_75 - percentile_25\n","\n","UR = percentile_75 + 1.5*IQR\n","LR = percentile_25 - 1.5*IQR\n","\n","df = df[(df[\"Quantity\"]<=UR) & (df[\"Quantity\"]>=LR) & (df[\"Quantity\"]>0)]\n","df.shape\n"]},{"cell_type":"markdown","metadata":{"id":"YIov6gnKtb23"},"source":["We calculated the upper range(UR) and lower range(LR) and removed all the values outside this range"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFOg2hKxlgzF"},"outputs":[],"source":["#removing outliers in Unit Price variable\n","percentile_75 = 4.13\n","percentile_25 = 1.25\n","\n","IQR = percentile_75 - percentile_25\n","\n","UR = percentile_75 + 1.5*IQR\n","LR = percentile_25 - 1.5*IQR\n","\n","df[\"UnitPrice\"] = df[\"UnitPrice\"].astype(float)\n","\n","df = df[(df[\"UnitPrice\"]<=UR) & (df[\"UnitPrice\"]>=LR) & (df[\"UnitPrice\"]>0)]\n","df.shape\n"]},{"cell_type":"markdown","metadata":{"id":"2tw5rQ9jugs3"},"source":["We calculated the upper range(UR) and lower range(LR) and removed all the values outside this range"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLwSitb_vkVa"},"outputs":[],"source":["#checking for any anomaly in country\n","df[\"Country\"].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"CA-XFITRv2Ew"},"source":["We did'nt found any outlier in other columns"]},{"cell_type":"markdown","metadata":{"id":"YiSIsnXT2MDA"},"source":["**4. Creating New Columns**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"xynI1WnC2qaP"},"outputs":[],"source":["#creating a new column total price\n","df[\"Total Price\"] = df['Quantity']*df['UnitPrice']\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0X6kxrgKZ5ny"},"outputs":[],"source":["df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A4FJWxnNaBxI"},"outputs":[],"source":["#removing outliers in Total Price\n","percentile_75 = 16.6\n","percentile_25 = 3.75\n","\n","IQR = percentile_75 - percentile_25\n","\n","UR = percentile_75 + 1.5*IQR\n","LR = percentile_25 - 1.5*IQR\n","\n","df = df[(df[\"Total Price\"]<=UR) & (df[\"Total Price\"]>=LR) & (df[\"Total Price\"]>0)]\n","df.shape\n"]},{"cell_type":"markdown","metadata":{"id":"7DTPFQIUa6rU"},"source":["Successfully removed outliers in Total Price"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"WD0ZUWyA3efE"},"outputs":[],"source":["#extracting year,month,day from invoice date\n","\n","df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"])\n","df[\"InvoiceYear\"] = df[\"InvoiceDate\"].dt.year\n","df[\"InvoiceMonth\"] = df[\"InvoiceDate\"].dt.month_name()\n","df[\"InvoiceDay\"] = df[\"InvoiceDate\"].dt.day_name()\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"MSa1f5Uengrz"},"source":["### What all manipulations have you done and insights you found?"]},{"cell_type":"markdown","metadata":{"id":"LbyXE7I1olp8"},"source":["Manipulations done -\n","\n","1. We have removed all rows having null values in 'InvoiceNo','Quantity', 'InvoiceDate, 'UnitPrice', 'CustomerID', 'Country'. We have removed a total of 135,086 rows\n","\n","2. We have removed 5225 duplicate entries\n","\n","3. We used interquartile range(IQR) method to remove 85511 rows having outliers in the columns quantity and unit price\n","\n","4. We have made a new column TotalPrice to find the total monetary value of each purchase. We made this by multiplying quantity and unit price\n","\n","5. Removed outliers in Total Price Variable using IQR method\n","\n","6. We made new columns InvoiceYear, InvoiceMonth, InvoiceDay to find the year, month and day of purchase"]},{"cell_type":"markdown","metadata":{"id":"GF8Ens_Soomf"},"source":["## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"]},{"cell_type":"markdown","metadata":{"id":"0wOQAZs5pc--"},"source":["#### Chart - 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7v_ESjsspbW7"},"outputs":[],"source":["# Chart - 1 visualization code\n","#plotting histogram of quantity\n","sns.histplot(data=df , x=df[\"Quantity\"])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"K5QZ13OEpz2H"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"XESiWehPqBRc"},"source":["Histogram is used to count the frequency of a continuous variable Quantity"]},{"cell_type":"markdown","metadata":{"id":"lQ7QKXXCp7Bj"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"C_j1G7yiqdRP"},"source":["Customers mostly buy products in the number from 1 to 12"]},{"cell_type":"markdown","metadata":{"id":"448CDAPjqfQr"},"source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."]},{"cell_type":"markdown","metadata":{"id":"3cspy4FjqxJW"},"source":["By knowing the quantity preferences of customers our clients can optimize their inventory thus increasing profits"]},{"cell_type":"markdown","metadata":{"id":"KSlN3yHqYklG"},"source":["#### Chart - 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4YgtaqtYklH"},"outputs":[],"source":["# Chart - 2 visualization code\n","#plotting histogram of unitprice\n","sns.histplot(data=df , x=df[\"UnitPrice\"])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"t6dVpIINYklI"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"5aaW0BYyYklI"},"source":["Histogram is used to plot the frequency of a single continuous variable UnitPrice"]},{"cell_type":"markdown","metadata":{"id":"ijmpgYnKYklI"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"PSx9atu2YklI"},"source":["Customers mostly buy products having unit price in the range of $ 0-6"]},{"cell_type":"markdown","metadata":{"id":"-JiQyfWJYklI"},"source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."]},{"cell_type":"markdown","metadata":{"id":"BcBbebzrYklV"},"source":["Our clients can optimize their inventory by preferring products in the most popular price range thus increasing their profit"]},{"cell_type":"markdown","metadata":{"id":"EM7whBJCYoAo"},"source":["#### Chart - 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t6GMdE67YoAp"},"outputs":[],"source":["# Chart - 3 visualization code\n","#plotting histogram of TotalPrice\n","sns.histplot(data=df , x=df[\"Total Price\"])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"fge-S5ZAYoAp"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"5dBItgRVYoAp"},"source":["Histogram is used to plot the frequency of a single continuous variable Total Price"]},{"cell_type":"markdown","metadata":{"id":"85gYPyotYoAp"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"4jstXR6OYoAp"},"source":["Mostly customers purchase in the price range of $ 0-20"]},{"cell_type":"markdown","metadata":{"id":"RoGjAbkUYoAp"},"source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."]},{"cell_type":"markdown","metadata":{"id":"zfJ8IqMcYoAp"},"source":["By knowing the price sensitivity of customers our clients can optimize their marketing strategies and inventories thus increasing their business prospects"]},{"cell_type":"markdown","metadata":{"id":"4Of9eVA-YrdM"},"source":["#### Chart - 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"irlUoxc8YrdO"},"outputs":[],"source":["# Chart - 4 visualization code\n","#plotting countplot of top 10 buying countries\n","countries = df[\"Country\"].value_counts() # series having count of countries in descending order\n","top_10_countries = countries[:10,] # series with top 10 buying countries\n","plt.figure(figsize = (14,8)) # adjusting figure size\n","sns.barplot(x = top_10_countries.index , y = top_10_countries.values )\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"iky9q4vBYrdO"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"aJRCwT6DYrdO"},"source":["Barplot is used to plot a categorical variable country against its counts in the dataset"]},{"cell_type":"markdown","metadata":{"id":"F6T5p64dYrdO"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"Xx8WAJvtYrdO"},"source":["United Kingdom far exceeds other countries in terms of purchases"]},{"cell_type":"markdown","metadata":{"id":"y-Ehk30pYrdP"},"source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."]},{"cell_type":"markdown","metadata":{"id":"jLNxxz7MYrdP"},"source":["Since most of the purchases are coming from the home country so the marketing strategies should be more focused on it rather than other countries"]},{"cell_type":"markdown","metadata":{"id":"bamQiAODYuh1"},"source":["#### Chart - 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TIJwrbroYuh3"},"outputs":[],"source":["# Chart - 5 visualization code\n","#plotting countplot of InvoiceYear\n","sns.countplot(x = df.InvoiceYear)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"QHF8YVU7Yuh3"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"dcxuIMRPYuh3"},"source":["Countplot is chosen to plot the counts of categorical variable InvoiceYear"]},{"cell_type":"markdown","metadata":{"id":"GwzvFGzlYuh3"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"uyqkiB8YYuh3"},"source":["Business has increased exponentially from 2010 to 2011"]},{"cell_type":"markdown","metadata":{"id":"qYpmQ266Yuh3"},"source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."]},{"cell_type":"markdown","metadata":{"id":"_WtzZ_hCYuh4"},"source":["Business and marketing strategies of our client are working fine looking at the growth of the company. They should carry forward their strategies."]},{"cell_type":"markdown","metadata":{"id":"OH-pJp9IphqM"},"source":["#### Chart - 6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kuRf4wtuphqN"},"outputs":[],"source":["# Chart - 6 visualization code\n","#plotting countplot of InvoiceMonth\n","plt.figure(figsize=(12,6))\n","sns.countplot(x = df.InvoiceMonth)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"bbFf2-_FphqN"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"loh7H2nzphqN"},"source":["Countplot is chosen to plot the counts of categorical variable InvoiceMonth"]},{"cell_type":"markdown","metadata":{"id":"_ouA3fa0phqN"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"VECbqPI7phqN"},"source":["Last 4 months sees the maximum sales"]},{"cell_type":"markdown","metadata":{"id":"Seke61FWphqN"},"source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."]},{"cell_type":"markdown","metadata":{"id":"DW4_bGpfphqN"},"source":["Inventories should be increased and marketing strategies directed towards the end of the year to increase profits"]},{"cell_type":"markdown","metadata":{"id":"PIIx-8_IphqN"},"source":["#### Chart - 7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqAIGUfyphqO"},"outputs":[],"source":["# Chart - 7 visualization code\n","#plotting country against average total price\n","data = df.groupby(\"Country\")[\"Total Price\"].mean() #grouping country and total price\n","data_sorted = data.sort_values(ascending=False).head(10) # sorting the prices in ascending order and finding top 10 countries\n","plt.figure(figsize=(15,6))\n","sns.barplot(x = data_sorted.index , y = data_sorted.values)\n","plt.ylabel(\"Mean Total Price\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"t27r6nlMphqO"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"iv6ro40sphqO"},"source":["Barplot is chosen to plot a categorical variable country against a continuous variable Mean Total Price"]},{"cell_type":"markdown","metadata":{"id":"r2jJGEOYphqO"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"Po6ZPi4hphqO"},"source":["Barring Czech Republic all other top buying countries have same mean total price."]},{"cell_type":"markdown","metadata":{"id":"b0JNsNcRphqO"},"source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."]},{"cell_type":"markdown","metadata":{"id":"xvSq8iUTphqO"},"source":["Like the previous recommendation of focusing on the home country this time also we got the insight that foreign buying is not up to the mark.\n","It is again recommended to focus on the marketing strategies and inventories in the home country"]},{"cell_type":"markdown","metadata":{"id":"BZR9WyysphqO"},"source":["#### Chart - 8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdPTWpAVphqO"},"outputs":[],"source":["# Chart - 8 visualization code\n","#plotting InvoiceYear against average total price\n","data = df.groupby(\"InvoiceYear\")[\"Total Price\"].mean()\n","data_sorted = data.sort_values(ascending=False).head(10)\n","sns.barplot(x = data_sorted.index , y = data_sorted.values)\n","plt.ylabel(\"Mean Total Price\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jj7wYXLtphqO"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"Ob8u6rCTphqO"},"source":["Barplot is chosen to plot a categorical variable InvoiceYear against a continuous variable Mean Total Price"]},{"cell_type":"markdown","metadata":{"id":"eZrbJ2SmphqO"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"mZtgC_hjphqO"},"source":["Mean Total price is almost the same for the years 2010 and 2011"]},{"cell_type":"markdown","metadata":{"id":"rFu4xreNphqO"},"source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."]},{"cell_type":"markdown","metadata":{"id":"ey_0qi68phqO"},"source":["Despite exponential increase in total sales in 2011 as compared to 2010 average sales is same for the years 2011 and 2010. It is advised to our clients to recaliberate their marketing strategies so that on an average the customers buy more."]},{"cell_type":"markdown","metadata":{"id":"YJ55k-q6phqO"},"source":["#### Chart - 9"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2aS4O1ophqO"},"outputs":[],"source":["# Chart - 9 visualization code\n","#plotting InvoiceMonth against mean total price\n","data = df.groupby(\"InvoiceMonth\")[\"Total Price\"].mean()\n","data_sorted = data.sort_values(ascending=False).head(10)\n","plt.figure(figsize = (10,6))\n","sns.barplot(x = data_sorted.index , y = data_sorted.values)\n","plt.ylabel(\"Mean Total Price\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"gCFgpxoyphqP"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"TVxDimi2phqP"},"source":["Barplot is chosen as we have to plot a categorical variable against a continuous variable"]},{"cell_type":"markdown","metadata":{"id":"OVtJsKN_phqQ"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"ngGi97qjphqQ"},"source":["Despite the last 4 months where most of the buying take place average prices still remain almost the same for every month  "]},{"cell_type":"markdown","metadata":{"id":"lssrdh5qphqQ"},"source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."]},{"cell_type":"markdown","metadata":{"id":"tBpY5ekJphqQ"},"source":["Like the previous insight our client must change their strategies so as to increase the average buying by customers."]},{"cell_type":"markdown","metadata":{"id":"U2RJ9gkRphqQ"},"source":["#### Chart - 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GM7a4YP4phqQ"},"outputs":[],"source":["# Chart - 10 visualization code\n","#plotting InvoiceDay against total price\n","data = df.groupby(\"InvoiceDay\")[\"Total Price\"].sum()\n","data_sorted = data.sort_values(ascending=False).head(10)\n","plt.figure(figsize = (10,6))\n","sns.barplot(x = data_sorted.index , y = data_sorted.values)\n","plt.ylabel(\"Total Price\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"1M8mcRywphqQ"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"8agQvks0phqQ"},"source":["Barplot is chosen to plot a categorical variable (InvoiceDay) against a continuous variable (Total Price)"]},{"cell_type":"markdown","metadata":{"id":"tgIPom80phqQ"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"Qp13pnNzphqQ"},"source":["Weekdays see more buying from customers as compared to weekends"]},{"cell_type":"markdown","metadata":{"id":"JMzcOPDDphqR"},"source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."]},{"cell_type":"markdown","metadata":{"id":"R4Ka1PC2phqR"},"source":["Unexpectedly weekdays see more buying than weekends. Weekends should be the most productive time for business. If this issue is sorted than business prospects can be greatly increased"]},{"cell_type":"markdown","metadata":{"id":"x-EpHcCOp1ci"},"source":["#### Chart - 11"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAQTIvtqp1cj"},"outputs":[],"source":["# Chart - 11 visualization code\n","#plotting unitprice with Quantity\n","sns.lineplot(x = df.Quantity, y = df.UnitPrice)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"X_VqEhTip1ck"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"-vsMzt_np1ck"},"source":["Linechart is chosen as both are continuous variables"]},{"cell_type":"markdown","metadata":{"id":"8zGJKyg5p1ck"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"ZYdMsrqVp1ck"},"source":["Unit price of product decreases as the quantity of the product increases"]},{"cell_type":"markdown","metadata":{"id":"PVzmfK_Ep1ck"},"source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."]},{"cell_type":"markdown","metadata":{"id":"druuKYZpp1ck"},"source":["There should be a check on the drastic decrease in unit price of products as their quantity increases. It should be checked whether any discount offered on buying more products is impacting the business prospects of the company."]},{"cell_type":"markdown","metadata":{"id":"n3dbpmDWp1ck"},"source":["#### Chart - 12"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwevp1tKp1ck"},"outputs":[],"source":["# Chart - 12 visualization code\n","# Plotting top 10 customers against total price\n","data = df.groupby(\"CustomerID\")[\"Total Price\"].sum()\n","data_sorted = data.sort_values(ascending=False).head(10)\n","plt.figure(figsize = (10,6))\n","sns.barplot(x = data_sorted.index , y = data_sorted.values)\n","plt.ylabel(\"Total Price\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ylSl6qgtp1ck"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"m2xqNkiQp1ck"},"source":["Barplot is chosen to plot a categorical variable CustomerId against a continuous variable total price"]},{"cell_type":"markdown","metadata":{"id":"ZWILFDl5p1ck"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"x-lUsV2mp1ck"},"source":["Our high value customers are spending significant amount on purchases with some customers having spent as high as $ 30000."]},{"cell_type":"markdown","metadata":{"id":"M7G43BXep1ck"},"source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."]},{"cell_type":"markdown","metadata":{"id":"5wwDJXsLp1cl"},"source":["These high value customers have a high potential of spending. Our client will greatly benefited if there are special marketing startegies for them"]},{"cell_type":"markdown","metadata":{"id":"Ag9LCva-p1cl"},"source":["#### Chart - 13"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUfxeq9-p1cl"},"outputs":[],"source":["# Chart - 13 visualization code\n","#plotting InvoiceMonth against total prices for different years\n","plt.figure(figsize = (12,5))\n","sns.barplot(x = df[\"InvoiceMonth\"] , y = df[\"Total Price\"]  , hue = df.InvoiceYear , estimator = np.sum)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"E6MkPsBcp1cl"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"V22bRsFWp1cl"},"source":["This chart is chosen to plot a categorical variable (InvoiceMonth) against continuous variable (Total Price)"]},{"cell_type":"markdown","metadata":{"id":"2cELzS2fp1cl"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"ozQPc2_Ip1cl"},"source":["We have plotted sales of different months across different years. Barring december there are no other months where sales are even comparable. 2011 saw exponential increase in sales"]},{"cell_type":"markdown","metadata":{"id":"3MPXvC8up1cl"},"source":["##### 3. Will the gained insights help creating a positive business impact?\n","Are there any insights that lead to negative growth? Justify with specific reason."]},{"cell_type":"markdown","metadata":{"id":"GL8l1tdLp1cl"},"source":["Client should continue the policies they have implemented after 2010 as that has resulted in multiple times increase in sales"]},{"cell_type":"markdown","metadata":{"id":"NC_X3p0fY2L0"},"source":["#### Chart - 14 - Correlation Heatmap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xyC9zolEZNRQ"},"outputs":[],"source":["# Correlation Heatmap visualization code\n","df_new = df[[\"Quantity\" , \"UnitPrice\" , \"Total Price\"]] #creating a new dataframe with only continuous variables\n","sns.heatmap(df_new.corr(), cmap=\"YlGnBu\", annot=True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"UV0SzAkaZNRQ"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"DVPuT8LYZNRQ"},"source":["Heatmap is used to plot the relationship between different continuous variables"]},{"cell_type":"markdown","metadata":{"id":"YPEH6qLeZNRQ"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"bfSqtnDqZNRR"},"source":["There is positive proportionality between quantity and total price and between unit price and total price. And there is negative proportionality between quantity and unit price"]},{"cell_type":"markdown","metadata":{"id":"q29F0dvdveiT"},"source":["#### Chart - 15 - Pair Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o58-TEIhveiU"},"outputs":[],"source":["# Pair Plot visualization code\n","plt.figure(figsize = (10,15))\n","sns.pairplot(df_new)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"EXh0U9oCveiU"},"source":["##### 1. Why did you pick the specific chart?"]},{"cell_type":"markdown","metadata":{"id":"eMmPjTByveiU"},"source":["Pair plot is chosen to plot graphs between different continuous variables"]},{"cell_type":"markdown","metadata":{"id":"22aHeOlLveiV"},"source":["##### 2. What is/are the insight(s) found from the chart?"]},{"cell_type":"markdown","metadata":{"id":"uPQ8RGwHveiV"},"source":["Quantity decreases as unit price increases\n","Total Price increases as unit price increases\n","Total Price increases as quantity increases"]},{"cell_type":"markdown","metadata":{"id":"g-ATYxFrGrvw"},"source":["## ***5. Hypothesis Testing***"]},{"cell_type":"markdown","metadata":{"id":"Yfr_Vlr8HBkt"},"source":["### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."]},{"cell_type":"markdown","metadata":{"id":"-7MS06SUHkB-"},"source":["We will perform hypothesis testing on three variables Quantity, Unit Price and Total Price. The different hypothesis we will perform is discussed below"]},{"cell_type":"markdown","metadata":{"id":"8yEUt7NnHlrM"},"source":["### Hypothetical Statement - 1"]},{"cell_type":"markdown","metadata":{"id":"tEA2Xm5dHt1r"},"source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."]},{"cell_type":"markdown","metadata":{"id":"HI9ZP0laH0D-"},"source":["Claim - Most of the product quantity bought by the individual customers are less than or equal to 12\n","\n","Null Hypothesis(Ho): p = 0.5\n","\n","Alternate Hypothesis(Ha): p < 0.5\n","\n","Here we take significance level = 0.05\n","If our p value comes below significance level then we reject the claim"]},{"cell_type":"markdown","metadata":{"id":"I79__PHVH19G"},"source":["#### 2. Perform an appropriate statistical test."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZrfquKtyian"},"outputs":[],"source":["# Perform Statistical Test to obtain P-Value\n","n = 1000 # number of samples taken\n","\n","df_sample = df.sample(n=1000) #taking sample of the dataset\n","\n","df_sample_quantity_less_than_equal_to_12 = df_sample[df_sample[\"Quantity\"]<=12] #finding entries with quantity less than 12 in df_sample\n","\n","#finding probability of entries with quantity less than 12 in df_sample\n","probability_quantity_less_than_equal_to_12 = len(df_sample_quantity_less_than_equal_to_12)/n\n","\n","probability_quantity_less_than_equal_to_12"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t4r7XtwZGdod"},"outputs":[],"source":["#finding test statistic z\n","p_sample  = probability_quantity_less_than_equal_to_12\n","p_claim = 0.5\n","q = 1- p_claim\n","\n","z = (p_sample - p_claim)/np.sqrt(p_claim*q/n)\n","z"]},{"cell_type":"markdown","metadata":{"id":"Yar95xfQBxef"},"source":["We see that for values of z = 3.50 and higher, we use 0.9999 for the cumulative area to the left of the test statistic. p value is 0.9999\n","p value is greater than 0.05 so our claim is true"]},{"cell_type":"markdown","metadata":{"id":"Ou-I18pAyIpj"},"source":["##### Which statistical test have you done to obtain P-Value?"]},{"cell_type":"markdown","metadata":{"id":"s2U0kk00ygSB"},"source":["Since our test statistic z is greater than 3.50 so we have taken our p value as 0.9999 which is standard for such z values"]},{"cell_type":"markdown","metadata":{"id":"fF3858GYyt-u"},"source":["##### Why did you choose the specific statistical test?"]},{"cell_type":"markdown","metadata":{"id":"HO4K0gP5y3B4"},"source":["We chose the specific statistical test as we have probability of sample and probability we claimed. We just have to find the test statistic z according to the formula above and find the p value corresponding to it. After measuring whether the p value is greater or less than the significance level (0.05) we accept or reject the null hypothesis"]},{"cell_type":"markdown","metadata":{"id":"4_0_7-oCpUZd"},"source":["### Hypothetical Statement - 2"]},{"cell_type":"markdown","metadata":{"id":"hwyV_J3ipUZe"},"source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."]},{"cell_type":"markdown","metadata":{"id":"FnpLGJ-4pUZe"},"source":["Claim - Most of the products have unit price less than 5\n","\n","Null Hypothesis(Ho): p = 0.5\n","\n","Alternate Hypothesis(Ha): p < 0.5\n","\n","Here we take significance level = 0.05\n","\n","If our p value comes below significance level then we reject the claim"]},{"cell_type":"markdown","metadata":{"id":"3yB-zSqbpUZe"},"source":["#### 2. Perform an appropriate statistical test."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sWxdNTXNpUZe"},"outputs":[],"source":["# Perform Statistical Test to obtain P-Value\n","n = 1000 # number of samples taken\n","\n","df_sample = df.sample(n=1000) #taking sample of the dataset\n","\n","df_sample_unitprice_less_than_5 = df_sample[df_sample[\"UnitPrice\"]<5] #finding entries with unit price less than 5 in df_sample\n","\n","#finding probability of entries with unit price less than 5 in df_sample\n","probability_unitprice_less_than_5 = len(df_sample_unitprice_less_than_5)/n\n","\n","probability_unitprice_less_than_5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_T-UNZ9TGXi"},"outputs":[],"source":["#finding test statistic z\n","p_sample  = probability_unitprice_less_than_5\n","p_claim = 0.5\n","q = 1- p_claim\n","\n","z = (p_sample - p_claim)/np.sqrt(p_claim*q/n)\n","z"]},{"cell_type":"markdown","metadata":{"id":"FZjleLf6TXy2"},"source":["We see that for values of z = 3.50 and higher, we use 0.9999 for the cumulative area to the left of the test statistic. p value is 0.9999. p value is greater than 0.05 so our claim is true"]},{"cell_type":"markdown","metadata":{"id":"dEUvejAfpUZe"},"source":["##### Which statistical test have you done to obtain P-Value?"]},{"cell_type":"markdown","metadata":{"id":"oLDrPz7HpUZf"},"source":["Since our test statistic z is greater than 3.50 so we have taken our p value as 0.9999 which is standard for such z values"]},{"cell_type":"markdown","metadata":{"id":"Fd15vwWVpUZf"},"source":["##### Why did you choose the specific statistical test?"]},{"cell_type":"markdown","metadata":{"id":"4xOGYyiBpUZf"},"source":["We chose the specific statistical test as we have probability of sample and probability we claimed. We just have to find the test statistic z according to the formula above and find the p value corresponding to it. After measuring whether the p value is greater or less than the significance level (0.05) we accept or reject the null hypothesis"]},{"cell_type":"markdown","metadata":{"id":"bn_IUdTipZyH"},"source":["### Hypothetical Statement - 3"]},{"cell_type":"markdown","metadata":{"id":"49K5P_iCpZyH"},"source":["#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."]},{"cell_type":"markdown","metadata":{"id":"7gWI5rT9pZyH"},"source":["Claim - Most of the sales have total price less than 30\n","\n","Null Hypothesis(Ho): p = 0.5\n","\n","Alternate Hypothesis(Ha): p < 0.5\n","\n","Here we take significance level = 0.05 If our p value comes below significance level then we reject the claim"]},{"cell_type":"markdown","metadata":{"id":"Nff-vKELpZyI"},"source":["#### 2. Perform an appropriate statistical test."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6AnJQjtpZyI"},"outputs":[],"source":["# Perform Statistical Test to obtain P-Value\n","n = 1000 # number of samples taken\n","\n","df_sample = df.sample(n=1000) #taking sample of the dataset\n","\n","df_sample_totalprice_less_than_30 = df_sample[df_sample[\"Total Price\"]<30] #finding entries with total price less than 30 in df_sample\n","\n","#finding probability of entries with total price less than 30 in df_sample\n","probability_totalprice_less_than_30 = len(df_sample_totalprice_less_than_30)/n\n","\n","probability_totalprice_less_than_30"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LNOGMRujWBog"},"outputs":[],"source":["#finding test statistic z\n","p_sample  = probability_totalprice_less_than_30\n","p_claim = 0.5\n","q = 1- p_claim\n","\n","z = (p_sample - p_claim)/np.sqrt(p_claim*q/n)\n","z"]},{"cell_type":"markdown","metadata":{"id":"0YJ21RuYWlXy"},"source":["We see that for values of z = 3.50 and higher, we use 0.9999 for the cumulative area to the left of the test statistic. p value is 0.9999. p value is greater than 0.05 so our claim is true"]},{"cell_type":"markdown","metadata":{"id":"kLW572S8pZyI"},"source":["##### Which statistical test have you done to obtain P-Value?"]},{"cell_type":"markdown","metadata":{"id":"ytWJ8v15pZyI"},"source":["Since our test statistic z is greater than 3.50 so we have taken our p value as 0.9999 which is standard for such z values"]},{"cell_type":"markdown","metadata":{"id":"dWbDXHzopZyI"},"source":["##### Why did you choose the specific statistical test?"]},{"cell_type":"markdown","metadata":{"id":"M99G98V6pZyI"},"source":["We chose the specific statistical test as we have probability of sample and probability we claimed. We just have to find the test statistic z according to the formula above and find the p value corresponding to it. After measuring whether the p value is greater or less than the significance level (0.05) we accept or reject the null hypothesis"]},{"cell_type":"markdown","metadata":{"id":"yLjJCtPM0KBk"},"source":["## ***6. Feature Engineering & Data Pre-processing***"]},{"cell_type":"markdown","metadata":{"id":"xiyOF9F70UgQ"},"source":["### 1. Handling Missing Values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iRsAHk1K0fpS"},"outputs":[],"source":["# Handling Missing Values & Missing Value Imputation"]},{"cell_type":"markdown","metadata":{"id":"bhfMI90ZXgK-"},"source":["Already handled missing values in data wrangling section"]},{"cell_type":"markdown","metadata":{"id":"7wuGOrhz0itI"},"source":["#### What all missing value imputation techniques have you used and why did you use those techniques?"]},{"cell_type":"markdown","metadata":{"id":"1ixusLtI0pqI"},"source":["Removed all rows having missing values in columns 'InvoiceNo','Quantity', 'InvoiceDate','UnitPrice', 'CustomerID', 'Country'. Used the dropna function for it. This techinque is used because these variables are important for our analysis"]},{"cell_type":"markdown","metadata":{"id":"id1riN9m0vUs"},"source":["### 2. Handling Outliers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6w2CzZf04JK"},"outputs":[],"source":["# Handling Outliers & Outlier treatments"]},{"cell_type":"markdown","metadata":{"id":"HJ9wMfb9YZnG"},"source":["Already handled outliers in data wrangling section"]},{"cell_type":"markdown","metadata":{"id":"578E2V7j08f6"},"source":["##### What all outlier treatment techniques have you used and why did you use those techniques?"]},{"cell_type":"markdown","metadata":{"id":"uGZz5OrT1HH-"},"source":["We removed outliers in Quantity,Unit Price and Total Price variables through IQR method. IQR is the difference between the value at 75 percentile to the value at 25 percentile. We found the upper range and lower range for our variables through the below formula\n","\n","Upper Range = 75 percentile value + 1.5IQR\n","\n","Lower Range = 25 percentile value - 1.5IQR\n","\n","We removed any value outside the upper and lower range\n","\n","This method is used as it is a reliable method to remove outliers in continuous variables"]},{"cell_type":"markdown","metadata":{"id":"89xtkJwZ18nB"},"source":["### 3. Categorical Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"21JmIYMG2hEo"},"outputs":[],"source":["# Encode your categorical columns"]},{"cell_type":"markdown","metadata":{"id":"RB5W3hQyKy_j"},"source":["There is no need for categorical encoding in our project"]},{"cell_type":"markdown","metadata":{"id":"67NQN5KX2AMe"},"source":["#### What all categorical encoding techniques have you used & why did you use those techniques?"]},{"cell_type":"markdown","metadata":{"id":"UDaue5h32n_G"},"source":["Answer Here."]},{"cell_type":"markdown","metadata":{"id":"Iwf50b-R2tYG"},"source":["### 4. Textual Data Preprocessing\n","(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"]},{"cell_type":"markdown","metadata":{"id":"EgBVg66PLO6_"},"source":["In our analysis and ML implementation we have not found any need for textual preprocessing"]},{"cell_type":"markdown","metadata":{"id":"GMQiZwjn3iu7"},"source":["#### 1. Expand Contraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTouz10C3oNN"},"outputs":[],"source":["# Expand Contraction"]},{"cell_type":"markdown","metadata":{"id":"WVIkgGqN3qsr"},"source":["#### 2. Lower Casing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88JnJ1jN3w7j"},"outputs":[],"source":["# Lower Casing"]},{"cell_type":"markdown","metadata":{"id":"XkPnILGE3zoT"},"source":["#### 3. Removing Punctuations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqbBqNaA33c0"},"outputs":[],"source":["# Remove Punctuations"]},{"cell_type":"markdown","metadata":{"id":"Hlsf0x5436Go"},"source":["#### 4. Removing URLs & Removing words and digits contain digits."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2sxKgKxu4Ip3"},"outputs":[],"source":["# Remove URLs & Remove words and digits contain digits"]},{"cell_type":"markdown","metadata":{"id":"mT9DMSJo4nBL"},"source":["#### 5. Removing Stopwords & Removing White spaces"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2LSJh154s8W"},"outputs":[],"source":["# Remove Stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EgLJGffy4vm0"},"outputs":[],"source":["# Remove White spaces"]},{"cell_type":"markdown","metadata":{"id":"c49ITxTc407N"},"source":["#### 6. Rephrase Text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foqY80Qu48N2"},"outputs":[],"source":["# Rephrase Text"]},{"cell_type":"markdown","metadata":{"id":"OeJFEK0N496M"},"source":["#### 7. Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijx1rUOS5CUU"},"outputs":[],"source":["# Tokenization"]},{"cell_type":"markdown","metadata":{"id":"9ExmJH0g5HBk"},"source":["#### 8. Text Normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AIJ1a-Zc5PY8"},"outputs":[],"source":["# Normalizing Text (i.e., Stemming, Lemmatization etc.)"]},{"cell_type":"markdown","metadata":{"id":"cJNqERVU536h"},"source":["##### Which text normalization technique have you used and why?"]},{"cell_type":"markdown","metadata":{"id":"Z9jKVxE06BC1"},"source":["Answer Here."]},{"cell_type":"markdown","metadata":{"id":"k5UmGsbsOxih"},"source":["#### 9. Part of speech tagging"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btT3ZJBAO6Ik"},"outputs":[],"source":["# POS Taging"]},{"cell_type":"markdown","metadata":{"id":"T0VqWOYE6DLQ"},"source":["#### 10. Text Vectorization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBRtdhth6JDE"},"outputs":[],"source":["# Vectorizing Text"]},{"cell_type":"markdown","metadata":{"id":"qBMux9mC6MCf"},"source":["##### Which text vectorization technique have you used and why?"]},{"cell_type":"markdown","metadata":{"id":"su2EnbCh6UKQ"},"source":["Answer Here."]},{"cell_type":"markdown","metadata":{"id":"-oLEiFgy-5Pf"},"source":["### 4. Feature Manipulation & Selection"]},{"cell_type":"markdown","metadata":{"id":"C74aWNz2AliB"},"source":["#### 1. Feature Manipulation"]},{"cell_type":"markdown","metadata":{"id":"RR3_aFjuLUPf"},"source":["We have created new features \"Total Features\" , \"InvoiceYear\" , \"InvoiceMonth\" , \"InvoiceDay\" for better understanding of customer behaviour"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h1qC4yhBApWC"},"outputs":[],"source":["# Manipulate Features to minimize feature correlation and create new features"]},{"cell_type":"markdown","metadata":{"id":"2DejudWSA-a0"},"source":["\n","   #### 2. Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"asoeRiSlMAza"},"source":["We will create new dataframes rfm_df and qp_df where we will create new features like \"Recency\", \"Frequency\",\"Monetary\",\"Quantity\"(Average), \"Total Price\"(Average). These features will be used in our model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YLhe8UmaBCEE"},"outputs":[],"source":["# Select your features wisely to avoid overfitting"]},{"cell_type":"markdown","metadata":{"id":"pEMng2IbBLp7"},"source":["##### What all feature selection methods have you used  and why?"]},{"cell_type":"markdown","metadata":{"id":"rb2Lh6Z8BgGs"},"source":["Answer Here."]},{"cell_type":"markdown","metadata":{"id":"rAdphbQ9Bhjc"},"source":["##### Which all features you found important and why?"]},{"cell_type":"markdown","metadata":{"id":"fGgaEstsBnaf"},"source":["Answer Here."]},{"cell_type":"markdown","metadata":{"id":"TNVZ9zx19K6k"},"source":["### 5. Data Transformation"]},{"cell_type":"markdown","metadata":{"id":"nqoHp30x9hH9"},"source":["#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"]},{"cell_type":"markdown","metadata":{"id":"EH00cp31N5iO"},"source":["We will take log of our variables before deploying them in the algorithm. This is the only transformation we will do. This transformation is done so as to balance the ranges of different variables and make them as close as possible to normal distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6quWQ1T9rtH"},"outputs":[],"source":["# Transform Your data\n"]},{"cell_type":"markdown","metadata":{"id":"rMDnDkt2B6du"},"source":["### 6. Data Scaling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dL9LWpySC6x_"},"outputs":[],"source":["# Scaling your data"]},{"cell_type":"markdown","metadata":{"id":"oXddFsPpMg6P"},"source":["We will use standard scaler to scale our data"]},{"cell_type":"markdown","metadata":{"id":"yiiVWRdJDDil"},"source":["##### Which method have you used to scale you data and why?"]},{"cell_type":"markdown","metadata":{"id":"fyLJ-O0hPcrL"},"source":["We will use standard scaler as it reduces the mean of a feature to 0 and the variance to 1 which significantly reduces the range of the feature and make it comparable to the other features where same scaling is deployed. This makes the model performance more accurate"]},{"cell_type":"markdown","metadata":{"id":"1UUpS68QDMuG"},"source":["### 7. Dimesionality Reduction"]},{"cell_type":"markdown","metadata":{"id":"DKX6Vk5oS3nr"},"source":["We have not used any dimensionality reduction technique"]},{"cell_type":"markdown","metadata":{"id":"kexQrXU-DjzY"},"source":["##### Do you think that dimensionality reduction is needed? Explain Why?"]},{"cell_type":"markdown","metadata":{"id":"GGRlBsSGDtTQ"},"source":["Answer Here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQfvxBBHDvCa"},"outputs":[],"source":["# DImensionality Reduction (If needed)"]},{"cell_type":"markdown","metadata":{"id":"T5CmagL3EC8N"},"source":["##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"]},{"cell_type":"markdown","metadata":{"id":"ZKr75IDuEM7t"},"source":["Answer Here."]},{"cell_type":"markdown","metadata":{"id":"BhH2vgX9EjGr"},"source":["### 8. Data Splitting"]},{"cell_type":"markdown","metadata":{"id":"zCy7_UZCTEV4"},"source":["In clustering we don't need to split the data into training and test data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0CTyd2UwEyNM"},"outputs":[],"source":["# Split your data to train and test. Choose Splitting ratio wisely."]},{"cell_type":"markdown","metadata":{"id":"qjKvONjwE8ra"},"source":["##### What data splitting ratio have you used and why?"]},{"cell_type":"markdown","metadata":{"id":"Y2lJ8cobFDb_"},"source":["Answer Here."]},{"cell_type":"markdown","metadata":{"id":"P1XJ9OREExlT"},"source":["### 9. Handling Imbalanced Dataset"]},{"cell_type":"markdown","metadata":{"id":"nkplXyKgTRa2"},"source":["We don't need to handle imbalanced data"]},{"cell_type":"markdown","metadata":{"id":"VFOzZv6IFROw"},"source":["##### Do you think the dataset is imbalanced? Explain Why."]},{"cell_type":"markdown","metadata":{"id":"GeKDIv7pFgcC"},"source":["Answer Here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQsRhhZLFiDs"},"outputs":[],"source":["# Handling Imbalanced Dataset (If needed)"]},{"cell_type":"markdown","metadata":{"id":"TIqpNgepFxVj"},"source":["##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"]},{"cell_type":"markdown","metadata":{"id":"qbet1HwdGDTz"},"source":["Answer Here."]},{"cell_type":"markdown","metadata":{"id":"VfCC591jGiD4"},"source":["## ***7. ML Model Implementation***"]},{"cell_type":"markdown","metadata":{"id":"OB4l2ZhMeS1U"},"source":["### ML Model - 1"]},{"cell_type":"markdown","metadata":{"id":"7-vwXNQzVRJ_"},"source":["Our first model is RFM (Recency, Frequency, Monetary Value) model\n","\n","Recency, frequency, monetary value is a marketing analysis tool used to identify a company's or an organization's best customers by measuring and analyzing spending habits.\n","\n","The RFM model is based on three quantitative factors:\n","\n","Recency: How recently a customer has made a purchase\n","\n","Frequency: How often a customer makes a purchase\n","\n","Monetary Value: How much money a customer spends on purchases"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ebyywQieS1U"},"outputs":[],"source":["# ML Model - 1 Implementation\n","\n","# Fit the Algorithm\n","\n","# Predict on the model\n","\n","# we will take the latest date as 2011-12-10 as the last invoice date was 2011-11-09\n","latest_date = dt.datetime(2011,12,10)\n","\n","# calculating days since last purchase , frequency of purchases and total price spent by each customer\n","rfm_df = df.groupby(\"CustomerID\").agg({\"InvoiceDate\" : lambda x : (latest_date - x.max()).days ,\n","                                       \"InvoiceNo\" : lambda x : len(x) , \"Total Price\" : lambda x : x.sum()})\n","\n","# renaming columns\n","rfm_df.rename(columns = {\"InvoiceDate\" : \"Recency\" , \"InvoiceNo\" : \"Frequency\" , \"Total Price\" : \"Monetary\"}, inplace = True)\n","\n","# changing datatype of recency\n","rfm_df[\"Recency\"] = rfm_df[\"Recency\"].astype(int)\n","rfm_df.reset_index().head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpDeSlUeeYgc"},"outputs":[],"source":["# descriptive stats of rfm_df\n","rfm_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"udtiQ-btp_o5"},"outputs":[],"source":["# plotting distribution of rfm values\n","fig,axes = plt.subplots(2,2 , figsize = (20,13))\n","sns.distplot(rfm_df[\"Recency\"] , ax = axes[0,0])\n","sns.distplot(rfm_df[\"Frequency\"] , ax = axes[0,1])\n","sns.distplot(rfm_df[\"Monetary\"] , ax = axes[1,0])\n","\n","axes[0,0].set_title(\"Recency Distribution\")\n","axes[0,1].set_title(\"Frequency Distribution\")\n","axes[1,0].set_title(\"Monetary Distribution\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3kNfj_CgzQyW"},"outputs":[],"source":["# applying log transformation in recency, frequency, monetary variables\n","rfm_df[\"Recency_log\"] = np.log(rfm_df[\"Recency\"])\n","rfm_df[\"Frequency_log\"] = np.log(rfm_df[\"Frequency\"])\n","rfm_df[\"Monetary_log\"] = np.log(rfm_df[\"Monetary\"])\n","rfm_df"]},{"cell_type":"markdown","metadata":{"id":"AH8DCm2YMVUP"},"source":["We applied log transformation in order to balance the ranges of the three variables and make the distribution of variables close to normal distribution.\n","We will now visualize the log transformation of variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YLdEzdUVV8rs"},"outputs":[],"source":["# removing non positive values from rfm_df before visualizing the log transformation\n","\n","rfm_df = rfm_df[(rfm_df[\"Recency\"]>0) & (rfm_df[\"Frequency\"]>0) & (rfm_df[\"Monetary\"]>0)]\n","rfm_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFgJBfztOjVg"},"outputs":[],"source":["fig,axes = plt.subplots(2,2 , figsize = (20,13))\n","sns.distplot(rfm_df[\"Recency_log\"] , ax = axes[0,0])\n","sns.distplot(rfm_df[\"Frequency_log\"] , ax = axes[0,1])\n","sns.distplot(rfm_df[\"Monetary_log\"] , ax = axes[1,0])\n","\n","axes[0,0].set_title(\"Recency_log Distribution\")\n","axes[0,1].set_title(\"Frequency_log Distribution\")\n","axes[1,0].set_title(\"Monetary_log Distribution\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"P21WsiMY0JdC"},"source":["Log transformation have resulted in almost normal distribution as we can see from the graphs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QjC3y-6Sw5Eb"},"outputs":[],"source":["# scaling the data\n","\n","features = rfm_df[[\"Recency_log\",\"Frequency_log\",\"Monetary_log\"]].values\n","scaler = StandardScaler()\n","X = scaler.fit_transform(features)"]},{"cell_type":"markdown","metadata":{"id":"cuJw_4RD1ulP"},"source":["****IMPLEMENTING K MEANS CLUSTERING****"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vlq9kKH018X6"},"outputs":[],"source":["#elbow method to find out the best number of clusters\n","\n","inertia = []\n","\n","for k in range(1,10):\n","  kmeans = KMeans(n_clusters = k , random_state = 0).fit(X)\n","  inertia.append(kmeans.inertia_)\n","\n","#plotting inertia with number of clusters\n","plt.figure(figsize=(8, 6))\n","sns.lineplot(x=range(1, 10),y=inertia, marker='o')\n","plt.title(\"Elbow Method\")\n","plt.xlabel(\"Number of clusters\")\n","plt.ylabel(\"Inertia\")\n","plt.xticks(range(1, 10))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"eP569v8Y6Z8J"},"source":["From the elbow method we got the optimal number of clusters as 3\n","\n","We will now run the model with number of clusters as 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZeVuZ4jP7K0g"},"outputs":[],"source":["kmeans = KMeans(n_clusters=3)\n","kmeans.fit(X)\n","y_km = kmeans.predict(X)\n","\n","# Plot the clusters\n","plt.figure(figsize=(12, 8))\n","plt.title('Customer Segmentation based on Recency and Frequency')\n","plt.scatter(X[:,0], X[:,1], c=y_km, s=50, cmap='Set2', label='Clusters')\n","\n","# Plot and annotate the centers\n","centers = kmeans.cluster_centers_\n","plt.scatter(centers[:,0], centers[:,1], c='red', s=210, alpha=0.6, marker='o')\n","for i, center in enumerate(centers):\n","    plt.annotate(f'Cluster {i}', (center[0], center[1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n","\n","plt.xlabel('Recency')\n","plt.ylabel('Frequency')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Qec9sWrmcxA2"},"source":["We can observe 3 clusters from when k means model is applied on the graph plotting recency and frequency"]},{"cell_type":"markdown","metadata":{"id":"FeVXu7xpgBSd"},"source":["**Splitting the RFM values into 4 quantiles**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"phxA00Y0gJjN"},"outputs":[],"source":["quantiles = rfm_df.quantile(q=[0.25,0.5,0.75])\n","quantiles = quantiles.to_dict()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UiQSMbEhYy4"},"outputs":[],"source":["#function to create RFM segments\n","\n","def RecencyScore(r):\n","  if r <= quantiles[\"Recency\"][0.25]:\n","    return 1\n","  elif r <= quantiles[\"Recency\"][0.50]:\n","    return 2\n","  elif r <= quantiles[\"Recency\"][0.75]:\n","    return 3\n","  else:\n","    return 4\n","\n","\n","\n","def FreqScore(r):\n","  if r <= quantiles[\"Frequency\"][0.25]:\n","    return 4\n","  elif r <= quantiles[\"Frequency\"][0.50]:\n","    return 3\n","  elif r <= quantiles[\"Frequency\"][0.75]:\n","    return 2\n","  else:\n","    return 1\n","\n","\n","def MontScore(r):\n","  if r <= quantiles[\"Monetary\"][0.25]:\n","    return 4\n","  elif r <= quantiles[\"Monetary\"][0.50]:\n","    return 3\n","  elif r <= quantiles[\"Monetary\"][0.75]:\n","    return 2\n","  else:\n","    return 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1HfPKxFjjOls"},"outputs":[],"source":["# calculate RFM segment values for each record\n","rfm_df['R'] = rfm_df['Recency'].apply(RecencyScore)\n","rfm_df['F'] = rfm_df['Frequency'].apply(FreqScore)\n","rfm_df['M'] = rfm_df['Monetary'].apply(MontScore)\n","rfm_df.reset_index().head()"]},{"cell_type":"markdown","metadata":{"id":"pKh_q4C2MojT"},"source":["**Calculating RFM score from RFM segementation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4taBkpnMxU9"},"outputs":[],"source":["rfm_df[\"RFMGroup\"] = rfm_df[\"R\"].map(str) + rfm_df[\"F\"].map(str) + rfm_df[\"M\"].map(str)\n","\n","rfm_df[\"RFMScore\"] = rfm_df[\"R\"] + rfm_df[\"F\"] + rfm_df[\"M\"]\n","\n","rfm_df.reset_index().head()"]},{"cell_type":"markdown","metadata":{"id":"shLbqZX9P1lf"},"source":["We got the RFM score for each datapoint. Now we will assign each datapoint to a cluster"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NyO5_W16Pz22"},"outputs":[],"source":["# assigning datapoints to clusters\n","rfm_df[\"Cluster\"] = kmeans.labels_\n","rfm_df.reset_index().head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kdgi7MmJRV9Q"},"outputs":[],"source":["# calculate mean values each cluster\n","cluster_average = rfm_df.groupby('Cluster').mean()\n","cluster_average"]},{"cell_type":"markdown","metadata":{"id":"5I3_4CN7pbYz"},"source":["**INTERPRETATION :**"]},{"cell_type":"markdown","metadata":{"id":"TJxO_wi5qedQ"},"source":["CLUSTER 0 :\n","\n","*   Recency - Moderate (103 days)\n","*   Frequency - Moderate (41)\n","\n","\n","*   Monetary - Moderate ( $ 480)\n","*   Interpretation - These are potential loyalists having moderate interaction with our client. They can be pursued to increase their purchases from the comapny and become permanent customers\n","\n","\n","CLUSTER 2 :\n","\n","*   Recency - High (160 days)\n","*   Frequency - Low (8)\n","\n","\n","*   Monetary - Low ($ 106)\n","*   Interpretation - These are occasional/part time buyers who need special attention from our client. They usually don't make purchases from our company so they need different marketing startegies\n","\n","\n","\n","CLUSTER 1 :\n","\n","*   Recency - Low (17 days)\n","*   Frequency - High (179)\n","\n","\n","*   Monetary - High ($ 1874)\n","*   Interpretation - These customers are loyalist/permanent customers who regularly make purchases from our client. They should be retained as they form the base of the business of our client"]},{"cell_type":"markdown","metadata":{"id":"HqcCq4cm7aOn"},"source":["NOTE - Cluster numbers change eveytime we run the code so the above cluster information should be seen for the position the clusters represent in the graph and not the numbers they have"]},{"cell_type":"markdown","metadata":{"id":"ArJBuiUVfxKd"},"source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."]},{"cell_type":"markdown","metadata":{"id":"07ujl_SH8jGY"},"source":["Here we have used the RFM (Recency, Frequency, Monetary) model for our clustering purposes.\n","\n","1. Recency is the amount of days passed since the customer made the last purchase\n","\n","2. Frequency is the number of times the customer made purchases\n","\n","3. Monetary is the total amount spent by the customer in making purchases\n","\n","First we have taken the log of these three variables and scaled them using standard scaler technique for better performance of our model.\n","\n","Here we have plotted recency against frequency and found the different clusters through KMeans algorithm which makes use of expectation maximization technique to clusterize the dataset.\n","\n","Elbow method predicted 3 as the most appropriate number of clusters for our task. Then we have calculated RFM scores for these clusters to deduce common information among each member of the 3 clusters\n","\n","These 3 clusters of data representing customer information can be used by our client to optimize and channelize their marketing strategies thus increasing their profit\n","\n","Now we will evaluate our model with silhouette score technique"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqD5ZohzfxKe"},"outputs":[],"source":["# Visualizing evaluation Metric Score chart\n","#silhouetee score visualization\n","\n","from yellowbrick.cluster import SilhouetteVisualizer\n","from sklearn.metrics import silhouette_score\n","\n","#calculating and visualizing silhouette scores for cluster numbers from 2-9\n","for n in range(2,10):\n","  kmeans = KMeans(n_clusters = n)\n","  y_pred = kmeans.fit_predict(X)\n","\n","  score = silhouette_score(X, y_pred, metric = 'euclidean')\n","\n","  print(f\"Silhouette score for {n} clusters is {score}\")\n","\n","  visualizer = SilhouetteVisualizer(kmeans)\n","  visualizer.fit(X)\n","  visualizer.poof()"]},{"cell_type":"markdown","metadata":{"id":"vy14jq90lhcf"},"source":["As the average silhouetee score of dataset increases and reaches close to 1 we can assume that our clustering is done right and vice versa for silhouetee score becoming 0 or negative.\n","\n","In the above charts our highest average silhouetee score is coming for clusters number = 2 but the elbow method we used earlier suggested that the appropriate cluster number is 3 for our task. Our next highest average silhouette score is also coming for clusters number = 3 so we have taken 3 as the most appropriate number of clusters required"]},{"cell_type":"markdown","metadata":{"id":"4qY1EAkEfxKe"},"source":["#### 2. Cross- Validation & Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dy61ujd6fxKe"},"outputs":[],"source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","# Fit the Algorithm\n","\n","# Predict on the model\n","\n","from sklearn.model_selection import KFold\n","\n","# Number of clusters\n","num_clusters = 3\n","\n","# Initialize K-Fold Cross-Validation\n","k = 5\n","kf = KFold(n_splits=k, shuffle=True, random_state=42)\n","\n","# Initialize a list to store silhouette scores\n","silhouette_scores = []\n","\n","# Iterate over the folds\n","for train_index, val_index in kf.split(X):\n","    # Split the data into training and validation sets\n","    X_train, X_val = X[train_index], X[val_index]\n","\n","    # Initialize and fit the KMeans model\n","    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n","    kmeans.fit(X_train)\n","\n","    # Predict cluster labels for validation data\n","    val_labels = kmeans.predict(X_val)\n","\n","    # Check if there is more than one unique cluster label\n","    if len(set(val_labels)) > 1:\n","        # Calculate silhouette score for the validation data\n","        score = silhouette_score(X_val, val_labels)\n","        silhouette_scores.append(score)\n","\n","# Handle case where no valid silhouette scores were calculated\n","if silhouette_scores:\n","    avg_silhouette_score = sum(silhouette_scores) / len(silhouette_scores)\n","    print(\"Silhouette Scores:\", silhouette_scores)\n","    print(\"Average Silhouette Score:\", avg_silhouette_score)\n","else:\n","    print(\"No valid silhouette scores were calculated.\")\n"]},{"cell_type":"markdown","metadata":{"id":"PiV4Ypx8fxKe"},"source":["##### Which hyperparameter optimization technique have you used and why?"]},{"cell_type":"markdown","metadata":{"id":"negyGRa7fxKf"},"source":["We have used k fold stratification technique to find the silhouette score for 5 different training and test sets. Through it we got five different silhouette scores.\n","\n","Averaging these 5 scores we got a more accurate silhouette score for our model than by training our data only one time  "]},{"cell_type":"markdown","metadata":{"id":"TfvqoZmBfxKf"},"source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."]},{"cell_type":"markdown","metadata":{"id":"OaLui8CcfxKf"},"source":["There is only marginal difference between our silhouette score and the score we got from k fold stratification technique. So there is no improvement\n","\n","Original Silhouette Score = 0.3196801458213756\n","\n","K fold Silhouette Score = 0.319018518855131"]},{"cell_type":"markdown","metadata":{"id":"dJ2tPlVmpsJ0"},"source":["### ML Model - 2"]},{"cell_type":"markdown","metadata":{"id":"kl8cCwQjHVOD"},"source":["Here we have used QP (Quantity,Total Price) model to evaluate the customers using mean quantity of products they buy and mean amount they spend on each transaction. Through it we divide our customers based on the revenue they generate for the company and marketing strategies can be optimized using such divisions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yci_sFPtyAU-"},"outputs":[],"source":["# ML Model - 3 Implementation\n","\n","# Fit the Algorithm\n","\n","# Predict on the model\n","\n","#creating a dataframe to map mean quantity and mean total price for each customer\n","qp_df = df.groupby(\"CustomerID\").agg({\"Quantity\" : lambda x : x.mean(), \"Total Price\" : lambda x : x.mean()})\n","qp_df.reset_index().head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIj2vuWUze9v"},"outputs":[],"source":["#plotting density plots for quantity and total price\n","fig,axes = plt.subplots(1,2 , figsize = (12,5))\n","sns.distplot(qp_df[\"Quantity\"] , ax = axes[0])\n","sns.distplot(qp_df[\"Total Price\"] , ax = axes[1])\n","\n","\n","axes[0].set_title(\"Quantity Distribution\")\n","axes[1].set_title(\"Total Price Distribution\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"yplfeqw1K9lW"},"source":["To make the above distributions as close to normal distribution as possible and make ranges of variables comparable we take log of them"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykBL20dK6srT"},"outputs":[],"source":["# taking log of variables\n","qp_df[\"Quantity_log\"] = np.log(qp_df[\"Quantity\"])\n","qp_df[\"TotalPrice_log\"] = np.log(qp_df[\"Total Price\"])\n","qp_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxrlg0bVLwNW"},"outputs":[],"source":["# plotting the logarithm of variables\n","fig,axes = plt.subplots(1,2 , figsize = (12,5))\n","sns.distplot(qp_df[\"Quantity_log\"] , ax = axes[0])\n","sns.distplot(qp_df[\"TotalPrice_log\"] , ax = axes[1])\n","\n","\n","axes[0].set_title(\"Quantity_log Distribution\")\n","axes[1].set_title(\"TotalPrice_log Distribution\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oeTzRP3tAQDK"},"outputs":[],"source":["# scaling the logarithm of variables using standard scaler\n","features = qp_df[[\"Quantity_log\",\"TotalPrice_log\"]].values\n","scaler = StandardScaler()\n","\n","X = scaler.fit_transform(features)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XBqOzNOdBhBt"},"outputs":[],"source":["# applying elbow method to find appropriate number of clusters\n","inertia = []\n","\n","for k in range(1,10):\n","  kmeans = KMeans(n_clusters = k , random_state = 0).fit(X)\n","  inertia.append(kmeans.inertia_)\n","\n","#plotting inertia with number of clusters\n","plt.figure(figsize=(8, 6))\n","sns.lineplot(x=range(1, 10),y=inertia, marker='o')\n","plt.title(\"Elbow Method\")\n","plt.xlabel(\"Number of clusters\")\n","plt.ylabel(\"Inertia\")\n","plt.xticks(range(1, 10))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"x0uGRYUwX6Za"},"source":["We got 3 as the most appropriate number for the clusters"]},{"cell_type":"markdown","metadata":{"id":"7VD3O3GVXq55"},"source":["**Applying KMeans Algorithm**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vRmrnkEvC_Eg"},"outputs":[],"source":["kmeans = KMeans(n_clusters=3)\n","kmeans.fit(X)\n","y_km = kmeans.predict(X)\n","\n","# Plot the clusters\n","plt.figure(figsize=(12, 8))\n","plt.title('Customer Segmentation based on Quantity and Total Price')\n","plt.scatter(X[:,0], X[:,1], c=y_km, s=50, cmap='Set2', label='Clusters')\n","\n","# Plot and annotate the centers\n","centers = kmeans.cluster_centers_\n","plt.scatter(centers[:,0], centers[:,1], c='red', s=210, alpha=0.6, marker='o')\n","for i, center in enumerate(centers):\n","    plt.annotate(f'Cluster {i}', (center[0], center[1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n","\n","plt.xlabel('Quantity')\n","plt.ylabel('Total Price')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"eZBIhsDOZAaf"},"source":["We can clearly visualize the 3 clusters in above graph plotting quantity and total price"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhVxgC5DFm6q"},"outputs":[],"source":["# dividing the dataset into 4 quantiles\n","quantiles = qp_df.quantile(q=[0.25,0.5,0.75])\n","quantiles = quantiles.to_dict()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Va3IRhDZF7Cd"},"outputs":[],"source":["# function to assign rank according to quantity of products bought by customers\n","def QuantityScore(r):\n","  if r <= quantiles[\"Quantity\"][0.25]:\n","    return 4\n","  elif r <= quantiles[\"Quantity\"][0.50]:\n","    return 3\n","  elif r <= quantiles[\"Quantity\"][0.75]:\n","    return 2\n","  else:\n","    return 1\n","\n","\n","# function to assign rank according to total price spent by customers\n","def TotalPriceScore(r):\n","  if r <= quantiles[\"Total Price\"][0.25]:\n","    return 4\n","  elif r <= quantiles[\"Total Price\"][0.50]:\n","    return 3\n","  elif r <= quantiles[\"Total Price\"][0.75]:\n","    return 2\n","  else:\n","    return 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mipq4h8TGvTz"},"outputs":[],"source":["# creating separate columns to assign ranks according to quantity and total price\n","qp_df['Q'] = qp_df['Quantity'].apply(QuantityScore)\n","qp_df['P'] = qp_df['Total Price'].apply(TotalPriceScore)\n","\n","qp_df.reset_index().head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0-OQruh4o7G"},"outputs":[],"source":["# creating two columns\n","qp_df[\"QP_Group\"] = qp_df[\"Q\"].map(str) + qp_df[\"P\"].map(str) # column to group different ranks of customers\n","qp_df[\"QP_Score\"] = qp_df[\"Q\"] + qp_df[\"P\"] # column to find sum of ranks\n","qp_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5t5qBtvJ6Daz"},"outputs":[],"source":["# assigning clusters to the customers\n","qp_df[\"Cluster\"] = kmeans.labels_\n","qp_df.reset_index().head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ws3S_Q6f71Cn"},"outputs":[],"source":["# finding average of variables for different clusters\n","qp_df_averages = qp_df.groupby(\"Cluster\").mean()\n","qp_df_averages"]},{"cell_type":"markdown","metadata":{"id":"fB20FstXcK2V"},"source":["**Interpretation** -"]},{"cell_type":"markdown","metadata":{"id":"5s1cdFNl_FC_"},"source":["Cluster 0:\n","\n","Quantity: Moderate (approx 4)\n","\n","Total Price: Moderate (approx $ 9)\n","\n","Interpretation: These customers are the ones responsible for ensuring moderate amount of revenue for our client because they buy products in moderate quantity as well the prices of products they buy are moderate. These should be preserved and marketing strategies should focus on ensuring it as well as making them buy more as they have the potential of becoming high value customers\n","\n","\n","\n","Cluster 1:\n","\n","Quantity: Low (approx 2)\n","\n","Total Price: Low (approx $ 4)\n","\n","Interpretation: These customers are low revenue generating customers as they buy low quantity of products and the prices of products are also low. These customers can be casual buyers and marketing startegies should focus on retaining them and making them buy more as they are not spending much. Special discounts can help in this case\n","\n","\n","Cluster 2:\n","\n","Quantity: High (approx 9)\n","\n","Total Price: High (approx $ 17)\n","\n","Interpretation: These customers are high value customers responsible for maximum profit for the company. They buy in more quantity and the prices of the product they buy are also high. They are loyal customers which must be retained at all cost. Marketing strategies should focus on esuring the same."]},{"cell_type":"markdown","metadata":{"id":"JWYfwnehpsJ1"},"source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."]},{"cell_type":"markdown","metadata":{"id":"oHc0inSarXSc"},"source":["We have used the QP (Quantity - Total Price) model where we grouped the customers based on the average quantity of products they buy and the average price they pay on each transaction.\n","\n","First we grouped each customer's quantity of products and total price and calculated the mean of it. Then we took the log of the above variables and scaled them according to Standard Scaler.\n","\n","After scaling we used the elbow method to find the optimal number of clusters for our model. This we have done by plotting quantity with total price\n","\n","We got 3 as the optimal number for clusters. Then we used k means algorithm to plot the two variables and visualize the 3 clusters\n","\n","Then we found the QP group and QP score according to the rankings we assigned based on the quantity and total price associated with each customer\n","\n","Each cluster characteristics and the marketing impact it can have are described after that"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEl-hgQWpsJ1"},"outputs":[],"source":["# Visualizing evaluation Metric Score chart.0\n","\n","#silhouetee score visualization\n","\n","from yellowbrick.cluster import SilhouetteVisualizer\n","from sklearn.metrics import silhouette_score\n","\n","#calculating and visualizing silhouette scores for cluster numbers from 2-9\n","for n in range(2,10):\n","  kmeans = KMeans(n_clusters = n)\n","  y_pred = kmeans.fit_predict(X)\n","\n","  score = silhouette_score(X, y_pred, metric = 'euclidean')\n","\n","  print(f\"Silhouette score for {n} clusters is {score}\")\n","\n","  visualizer = SilhouetteVisualizer(kmeans)\n","  visualizer.fit(X)\n","  visualizer.poof()"]},{"cell_type":"markdown","metadata":{"id":"chvAFOQwWGzN"},"source":["Based on the elbow method we used previously and silhouette score we calculated for different clusters we found the optimal number of clusters to be 3 only"]},{"cell_type":"markdown","metadata":{"id":"-jK_YjpMpsJ2"},"source":["#### 2. Cross- Validation & Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dn0EOfS6psJ2"},"outputs":[],"source":["# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","# Fit the Algorithm\n","\n","# Predict on the model\n","\n","from sklearn.model_selection import KFold\n","\n","# Number of clusters\n","num_clusters = 3\n","\n","# Initialize K-Fold Cross-Validation\n","k = 5\n","kf = KFold(n_splits=k, shuffle=True, random_state=42)\n","\n","# Initialize a list to store silhouette scores\n","silhouette_scores = []\n","\n","# Iterate over the folds\n","for train_index, val_index in kf.split(X):\n","    # Split the data into training and validation sets\n","    X_train, X_val = X[train_index], X[val_index]\n","\n","    # Initialize and fit the KMeans model\n","    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n","    kmeans.fit(X_train)\n","\n","    # Predict cluster labels for validation data\n","    val_labels = kmeans.predict(X_val)\n","\n","    # Check if there is more than one unique cluster label\n","    if len(set(val_labels)) > 1:\n","        # Calculate silhouette score for the validation data\n","        score = silhouette_score(X_val, val_labels)\n","        silhouette_scores.append(score)\n","\n","# Handle case where no valid silhouette scores were calculated\n","if silhouette_scores:\n","    avg_silhouette_score = sum(silhouette_scores) / len(silhouette_scores)\n","    print(\"Silhouette Scores:\", silhouette_scores)\n","    print(\"Average Silhouette Score:\", avg_silhouette_score)\n","else:\n","    print(\"No valid silhouette scores were calculated.\")\n"]},{"cell_type":"markdown","metadata":{"id":"EGKRAURbW_zt"},"source":["We got average silhouette score to be 0.5777479124660421"]},{"cell_type":"markdown","metadata":{"id":"HAih1iBOpsJ2"},"source":["##### Which hyperparameter optimization technique have you used and why?"]},{"cell_type":"markdown","metadata":{"id":"9kBgjYcdpsJ2"},"source":["We have used k fold stratification technique to find the silhouette score for 5 different training and test sets. Through it we got five different silhouette scores.\n","\n","Averaging these 5 scores we got a more accurate silhouette score for our model than by training our data only one time"]},{"cell_type":"markdown","metadata":{"id":"zVGeBEFhpsJ2"},"source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."]},{"cell_type":"markdown","metadata":{"id":"74yRdG6UpsJ3"},"source":["There is only marginal difference between our silhouette score and the score we got from k fold stratification technique. So there is no improvement\n","\n","Original Silhouette Score = 0.5774089325283756\n","\n","K fold Silhouette Score = 0.5777479124660421"]},{"cell_type":"markdown","metadata":{"id":"bmKjuQ-FpsJ3"},"source":["#### 3. Explain each evaluation metric's indication towards business and the business impact of the ML model used."]},{"cell_type":"markdown","metadata":{"id":"BDKtOrBQpsJ3"},"source":["By finding the optimal number of clusters through Elbow and silhouetee score method we got the perfect grouping of our customers. This grouping can help our client know better about the customers and accordingly plan marketing strategies\n","This will go a long way in optimising business costs and increasing profits simultaneously."]},{"cell_type":"markdown","metadata":{"id":"Fze-IPXLpx6K"},"source":["### ML Model - 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFrSXAtrpx6M"},"outputs":[],"source":["# ML Model - 3 Implementation\n","\n","# Fit the Algorithm\n","\n","# Predict on the model"]},{"cell_type":"markdown","metadata":{"id":"7AN1z2sKpx6M"},"source":["#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIY4lxxGpx6M"},"outputs":[],"source":["# Visualizing evaluation Metric Score chart"]},{"cell_type":"markdown","metadata":{"id":"9PIHJqyupx6M"},"source":["#### 2. Cross- Validation & Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSVXuaSKpx6M"},"outputs":[],"source":["# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n","\n","# Fit the Algorithm\n","\n","# Predict on the model"]},{"cell_type":"markdown","metadata":{"id":"_-qAgymDpx6N"},"source":["##### Which hyperparameter optimization technique have you used and why?"]},{"cell_type":"markdown","metadata":{"id":"lQMffxkwpx6N"},"source":["Answer Here."]},{"cell_type":"markdown","metadata":{"id":"Z-hykwinpx6N"},"source":["##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."]},{"cell_type":"markdown","metadata":{"id":"MzVzZC6opx6N"},"source":["Answer Here."]},{"cell_type":"markdown","metadata":{"id":"h_CCil-SKHpo"},"source":["### 1. Which Evaluation metrics did you consider for a positive business impact and why?"]},{"cell_type":"markdown","metadata":{"id":"jHVz9hHDKFms"},"source":["Silhouetee score gave us the best evaluation mechanism to deduce the efficiency of the number of clusters we got through elbow method. This helped create perfect groups to optimise and channelize marketing strategies"]},{"cell_type":"markdown","metadata":{"id":"cBFFvTBNJzUa"},"source":["### 2. Which ML model did you choose from the above created models as your final prediction model and why?"]},{"cell_type":"markdown","metadata":{"id":"6ksF5Q1LKTVm"},"source":["RFM (Recency,Frequency,Monetary) model is the better model to optimize marketing strategies as it takes into account wholesome customer behaviour i.e. last purchase, total purchases and total amount spent. Through it the marketing costs are optimized in a substantial way as compared to the other model which takes into account just the average quantity and average price spent by each customer"]},{"cell_type":"markdown","metadata":{"id":"HvGl1hHyA_VK"},"source":["### 3. Explain the model which you have used and the feature importance using any model explainability tool?"]},{"cell_type":"markdown","metadata":{"id":"YnvVTiIxBL-C"},"source":["We have used 2 models here to segment our customers. Our first one is based on customer's wholesome behaviour in terms of his buying habits i.e. last purchase, total purchases in terms of number, total purchases in terms of value. Our second model is based on customer's buying habits in terms of average quantity of products purchased, average price of each transaction. This model also gives us a way to segment customers. These 2 models will help in optimising marketing costs and increase profits"]},{"cell_type":"markdown","metadata":{"id":"EyNgTHvd2WFk"},"source":["## ***8.*** ***Future Work (Optional)***"]},{"cell_type":"markdown","metadata":{"id":"KH5McJBi2d8v"},"source":["### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQIANRl32f4J"},"outputs":[],"source":["# Save the File"]},{"cell_type":"markdown","metadata":{"id":"iW_Lq9qf2h6X"},"source":["### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oEXk9ydD2nVC"},"outputs":[],"source":["# Load the File and predict unseen data."]},{"cell_type":"markdown","metadata":{"id":"-Kee-DAl2viO"},"source":["### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"]},{"cell_type":"markdown","metadata":{"id":"gCX9965dhzqZ"},"source":["# **Conclusion**"]},{"cell_type":"markdown","metadata":{"id":"Fjb1IsQkh3yE"},"source":["In this project we have created 2 models to segment our customers. These models used different strategies so as to make our client aware about the customers behaviour by grouping them into different segments and in this way they can plan their marketing and business strategies. We hope this project is successful in completing the task provided."]},{"cell_type":"markdown","metadata":{"id":"gIfDvo9L0UH2"},"source":["### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"]}],"metadata":{"colab":{"collapsed_sections":["vncDsAP0Gaoa","FJNUwmbgGyua","w6K7xa23Elo4","yQaldy8SH6Dl","mDgbUHAGgjLW","HhfV-JJviCcP","Y3lxredqlCYt","3RnN4peoiCZX","x71ZqKXriCWQ","7hBIi_osiCS2","JlHwYmJAmNHm","35m5QtbWiB9F","PoPl-ycgm1ru","H0kj-8xxnORC","nA9Y7ga8ng1Z","PBTbrJXOngz2","u3PMJOP6ngxN","GF8Ens_Soomf","0wOQAZs5pc--","K5QZ13OEpz2H","lQ7QKXXCp7Bj","448CDAPjqfQr","KSlN3yHqYklG","t6dVpIINYklI","ijmpgYnKYklI","-JiQyfWJYklI","EM7whBJCYoAo","fge-S5ZAYoAp","85gYPyotYoAp","RoGjAbkUYoAp","4Of9eVA-YrdM","iky9q4vBYrdO","F6T5p64dYrdO","y-Ehk30pYrdP","bamQiAODYuh1","QHF8YVU7Yuh3","GwzvFGzlYuh3","qYpmQ266Yuh3","OH-pJp9IphqM","bbFf2-_FphqN","_ouA3fa0phqN","Seke61FWphqN","PIIx-8_IphqN","t27r6nlMphqO","r2jJGEOYphqO","b0JNsNcRphqO","BZR9WyysphqO","jj7wYXLtphqO","eZrbJ2SmphqO","rFu4xreNphqO","YJ55k-q6phqO","gCFgpxoyphqP","OVtJsKN_phqQ","lssrdh5qphqQ","U2RJ9gkRphqQ","1M8mcRywphqQ","tgIPom80phqQ","JMzcOPDDphqR","x-EpHcCOp1ci","X_VqEhTip1ck","8zGJKyg5p1ck","PVzmfK_Ep1ck","n3dbpmDWp1ck","ylSl6qgtp1ck","ZWILFDl5p1ck","M7G43BXep1ck","Ag9LCva-p1cl","E6MkPsBcp1cl","2cELzS2fp1cl","3MPXvC8up1cl","NC_X3p0fY2L0","UV0SzAkaZNRQ","YPEH6qLeZNRQ","q29F0dvdveiT","EXh0U9oCveiU","22aHeOlLveiV","g-ATYxFrGrvw","Yfr_Vlr8HBkt","8yEUt7NnHlrM","tEA2Xm5dHt1r","I79__PHVH19G","Ou-I18pAyIpj","fF3858GYyt-u","4_0_7-oCpUZd","hwyV_J3ipUZe","3yB-zSqbpUZe","dEUvejAfpUZe","Fd15vwWVpUZf","bn_IUdTipZyH","49K5P_iCpZyH","Nff-vKELpZyI","kLW572S8pZyI","dWbDXHzopZyI","yLjJCtPM0KBk","xiyOF9F70UgQ","7wuGOrhz0itI","id1riN9m0vUs","578E2V7j08f6","89xtkJwZ18nB","67NQN5KX2AMe","Iwf50b-R2tYG","GMQiZwjn3iu7","WVIkgGqN3qsr","XkPnILGE3zoT","Hlsf0x5436Go","mT9DMSJo4nBL","c49ITxTc407N","OeJFEK0N496M","9ExmJH0g5HBk","cJNqERVU536h","k5UmGsbsOxih","T0VqWOYE6DLQ","qBMux9mC6MCf","-oLEiFgy-5Pf","C74aWNz2AliB","2DejudWSA-a0","pEMng2IbBLp7","rAdphbQ9Bhjc","TNVZ9zx19K6k","nqoHp30x9hH9","rMDnDkt2B6du","yiiVWRdJDDil","1UUpS68QDMuG","kexQrXU-DjzY","T5CmagL3EC8N","BhH2vgX9EjGr","qjKvONjwE8ra","P1XJ9OREExlT","VFOzZv6IFROw","TIqpNgepFxVj","JWYfwnehpsJ1","-jK_YjpMpsJ2","HAih1iBOpsJ2","zVGeBEFhpsJ2","bmKjuQ-FpsJ3","7AN1z2sKpx6M","9PIHJqyupx6M","_-qAgymDpx6N","Z-hykwinpx6N","h_CCil-SKHpo","cBFFvTBNJzUa","HvGl1hHyA_VK","EyNgTHvd2WFk","KH5McJBi2d8v","iW_Lq9qf2h6X","-Kee-DAl2viO","gCX9965dhzqZ","gIfDvo9L0UH2"],"private_outputs":true,"provenance":[{"file_id":"10B2s5af8INkHgIz_7XBjW3iNDbGY7ltx","timestamp":1715516943858}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
